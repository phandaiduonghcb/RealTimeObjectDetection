{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Article_headlines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRjxLTFPBthC"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP67jZUJMb8g"
      },
      "source": [
        "#Hàm này nhận vào headline, link, is_sarcastic rồi return ra chuỗi cho đúng format như file json mẫu\n",
        "def line(article,link,is_sarcastic):\n",
        "\n",
        "  line = '{\"article_link\": ' + '\"' + link + '\", \"headline\": ' + article.lower() + ', \"is_sarcastic\": ' + str(is_sarcastic) + '}'\n",
        "  return line"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuWMhJoQ35mD"
      },
      "source": [
        "def babylonbee_articles(n_pages):\n",
        "  f = open('babylonbee.json','a')\n",
        "  for i in range(1,n_pages+1):\n",
        "    site= \"https://babylonbee.com/news?page=\" + str(i)\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    page = urlopen(req)\n",
        "    soup = BeautifulSoup(page)\n",
        "    #print(soup)\n",
        "    containers = soup.find_all(\"article-card\")\n",
        "    for article_card in containers:\n",
        "      article_link = 'https://babylonbee.com' + article_card[\":path\"][1:-1]\n",
        "      headline = article_card[\":title\"]\n",
        "      f.write(line(headline,article_link,1) + '\\n')\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmUXpDMpBiAr"
      },
      "source": [
        "babylonbee_articles(354)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt2myrJzFVJ4"
      },
      "source": [
        "def thehardtimes_articles(n_pages):\n",
        "  f = open('thehardtimes.json','a')\n",
        "  for i in range(1,n_pages+1):\n",
        "    site= \"https://thehardtimes.net/page/\" + str(i) + \"/\"\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    page = urlopen(req)\n",
        "    soup = BeautifulSoup(page)\n",
        "    #print(soup)\n",
        "    containers = soup.find_all(\"div\",{\"class\":\"featured-image\"})\n",
        "    for article_card in containers:\n",
        "      h = '\"' + article_card.a.text + '\"'\n",
        "      l = article_card.a[\"href\"]\n",
        "      f.write(line(h,l,1) + '\\n')\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olf5mBJLGPY6"
      },
      "source": [
        "thehardtimes_articles(371)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCK5o-jvJdss",
        "outputId": "17009383-5745-411f-da61-23228a4d7d77"
      },
      "source": [
        "f = open('newyorktimes.json','a')\n",
        "for y in range(2020,2015,-1):\n",
        "  for m in range(12,0,-1):\n",
        "    for d in range(31,0,-1):\n",
        "      month=''\n",
        "      day =''\n",
        "      if m <10:\n",
        "        month = '0'+str(m)\n",
        "      else:\n",
        "        month = str(m)\n",
        "      if d <10:\n",
        "        day = '0'+str(d)\n",
        "      else:\n",
        "        day = str(d)\n",
        "      site= \"https://www.nytimes.com/sitemap/\" + str(y) + \"/\" + month + \"/\" + day+\"/\"\n",
        "      hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "      req = Request(site,headers=hdr)\n",
        "      try:\n",
        "        page = urlopen(req)\n",
        "        soup = BeautifulSoup(page)\n",
        "        #print(soup)\n",
        "        containers = soup.find_all(\"ul\")\n",
        "        articles = containers[0].find_all(\"li\")\n",
        "        for li in articles:\n",
        "          h = '\"' + li.a.text + '\"'\n",
        "          l = li.a[\"href\"]\n",
        "          f.write(line(h,l,0) + '\\n')\n",
        "      except:\n",
        "        print(site)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.nytimes.com/sitemap/2020/11/31/\n",
            "https://www.nytimes.com/sitemap/2020/09/31/\n",
            "https://www.nytimes.com/sitemap/2020/06/31/\n",
            "https://www.nytimes.com/sitemap/2020/04/31/\n",
            "https://www.nytimes.com/sitemap/2020/02/31/\n",
            "https://www.nytimes.com/sitemap/2020/02/30/\n",
            "https://www.nytimes.com/sitemap/2019/11/31/\n",
            "https://www.nytimes.com/sitemap/2019/09/31/\n",
            "https://www.nytimes.com/sitemap/2019/06/31/\n",
            "https://www.nytimes.com/sitemap/2019/04/31/\n",
            "https://www.nytimes.com/sitemap/2019/02/31/\n",
            "https://www.nytimes.com/sitemap/2019/02/30/\n",
            "https://www.nytimes.com/sitemap/2019/02/29/\n",
            "https://www.nytimes.com/sitemap/2018/11/31/\n",
            "https://www.nytimes.com/sitemap/2018/09/31/\n",
            "https://www.nytimes.com/sitemap/2018/06/31/\n",
            "https://www.nytimes.com/sitemap/2018/04/31/\n",
            "https://www.nytimes.com/sitemap/2018/02/31/\n",
            "https://www.nytimes.com/sitemap/2018/02/30/\n",
            "https://www.nytimes.com/sitemap/2018/02/29/\n",
            "https://www.nytimes.com/sitemap/2017/11/31/\n",
            "https://www.nytimes.com/sitemap/2017/09/31/\n",
            "https://www.nytimes.com/sitemap/2017/06/31/\n",
            "https://www.nytimes.com/sitemap/2017/04/31/\n",
            "https://www.nytimes.com/sitemap/2017/02/31/\n",
            "https://www.nytimes.com/sitemap/2017/02/30/\n",
            "https://www.nytimes.com/sitemap/2017/02/29/\n",
            "https://www.nytimes.com/sitemap/2016/11/31/\n",
            "https://www.nytimes.com/sitemap/2016/09/31/\n",
            "https://www.nytimes.com/sitemap/2016/06/31/\n",
            "https://www.nytimes.com/sitemap/2016/04/31/\n",
            "https://www.nytimes.com/sitemap/2016/02/31/\n",
            "https://www.nytimes.com/sitemap/2016/02/30/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEjvFx2V_Jpt",
        "outputId": "67f51072-a11b-4a6e-d863-26590ab72487"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import sys,time,random\n",
        "f = open('thetimescouk.json','a')\n",
        "for y in range(2020,2015,-1):\n",
        "  for m in range(12,0,-1):\n",
        "    for d in range(1,4):\n",
        "      month=''\n",
        "      if m <10:\n",
        "        month = '0'+str(m)\n",
        "      else:\n",
        "        month = str(m)\n",
        "      site= \"https://www.thetimes.co.uk/html-sitemap/\" + str(y) + \"-\" + month + \"-\" + str(d)\n",
        "      hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "      req = Request(site,headers=hdr)\n",
        "      try:\n",
        "        page = urlopen(req)\n",
        "        time.sleep(random.randint(5,10))\n",
        "        soup = BeautifulSoup(page)\n",
        "        containers = soup.find_all(\"ul\",{'class':'Sitemap-links'})\n",
        "        for ul in containers:\n",
        "          lists = ul.find_all(\"li\")\n",
        "          for li in lists:\n",
        "            h = '\"' + li.a.text + '\"'\n",
        "            l = \"https://www.thetimes.co.uk\" + li.a[\"href\"]\n",
        "            f.write(line(h,l,0) + '\\n')\n",
        "      except:\n",
        "        print(site)\n",
        "    print(month + '-' + str(y))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12-2020\n",
            "11-2020\n",
            "10-2020\n",
            "09-2020\n",
            "08-2020\n",
            "07-2020\n",
            "06-2020\n",
            "05-2020\n",
            "04-2020\n",
            "03-2020\n",
            "02-2020\n",
            "01-2020\n",
            "12-2019\n",
            "11-2019\n",
            "10-2019\n",
            "09-2019\n",
            "08-2019\n",
            "07-2019\n",
            "06-2019\n",
            "05-2019\n",
            "04-2019\n",
            "03-2019\n",
            "02-2019\n",
            "01-2019\n",
            "12-2018\n",
            "11-2018\n",
            "10-2018\n",
            "09-2018\n",
            "08-2018\n",
            "07-2018\n",
            "06-2018\n",
            "05-2018\n",
            "04-2018\n",
            "03-2018\n",
            "02-2018\n",
            "01-2018\n",
            "12-2017\n",
            "11-2017\n",
            "10-2017\n",
            "09-2017\n",
            "08-2017\n",
            "07-2017\n",
            "06-2017\n",
            "05-2017\n",
            "04-2017\n",
            "03-2017\n",
            "02-2017\n",
            "01-2017\n",
            "12-2016\n",
            "11-2016\n",
            "10-2016\n",
            "09-2016\n",
            "08-2016\n",
            "07-2016\n",
            "06-2016\n",
            "05-2016\n",
            "04-2016\n",
            "03-2016\n",
            "02-2016\n",
            "01-2016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H47IIeZ3N-al",
        "outputId": "f3f80ba5-66cf-4777-da3b-5d48912ccb80"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import sys,time,random\n",
        "f = open(\"ftcom.json\",'w')\n",
        "urls = [\"https://www.ft.com/world?page=\",\"https://www.ft.com/companies?page=\",\"https://www.ft.com/technology?page=\",\"https://www.ft.com/markets?page=\",\"https://www.ft.com/climate-capital?page=\",\"https://www.ft.com/opinion?page=\",\"https://www.ft.com/work-careers?page=\",\"https://www.ft.com/arts?page=\",\"https://www.ft.com/books?page=\",\"https://www.ft.com/food-drink?page=\",\"https://www.ft.com/magazine?page=\",\"https://www.ft.com/house-home?page=\",\"https://www.ft.com/style?page=\",\"https://www.ft.com/travel?page=\",\"https://www.ft.com/htsi?page=\"]\n",
        "i=1\n",
        "for url in urls:\n",
        "  while 1:\n",
        "    site = url + str(i)\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    try:\n",
        "      page = urlopen(req)\n",
        "      soup = BeautifulSoup(page)\n",
        "\n",
        "      t = soup.find_all('time')\n",
        "      if t[-1].text[-4:] == '2016':\n",
        "        break\n",
        "      a_ = soup.find_all(\"a\",{\"class\":\"js-teaser-heading-link\"})\n",
        "      for a in a_:\n",
        "        h = '\"' + a.text+ '\"'\n",
        "        l = \"https://www.ft.com\" +  a[\"href\"]\n",
        "        f.write(line(h,l,0) + '\\n')\n",
        "        \n",
        "    except:\n",
        "      print(site)\n",
        "      i=1\n",
        "      break\n",
        "    i+=1\n",
        "f.close()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.ft.com/world?page=201\n",
            "https://www.ft.com/companies?page=201\n",
            "https://www.ft.com/technology?page=201\n",
            "https://www.ft.com/markets?page=201\n",
            "https://www.ft.com/opinion?page=201\n",
            "https://www.ft.com/work-careers?page=201\n",
            "https://www.ft.com/arts?page=201\n",
            "https://www.ft.com/food-drink?page=201\n",
            "https://www.ft.com/style?page=201\n",
            "https://www.ft.com/htsi?page=75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10aGVe-P6730",
        "outputId": "9f3f7b50-7c46-41f6-d590-f1ce64df43af"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import sys,time,random\n",
        "f = open(\"theguardiancom.json\",'w')\n",
        "urls = [\"https://www.theguardian.com/world?page=\",\"https://www.theguardian.com/commentisfree?page=\",\"https://www.theguardian.com/sport?page=\",\"https://www.theguardian.com/culture?page=\",\"https://www.theguardian.com/lifeandstyle?page=\"]\n",
        "i=1\n",
        "for url in urls:\n",
        "  while 1:\n",
        "    site = url + str(i)\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    try:\n",
        "      page = urlopen(req)\n",
        "      soup = BeautifulSoup(page)\n",
        "\n",
        "      t = soup.find_all(\"time\",{\"class\":\"fc-date-headline\"})\n",
        "      if t[-1].text[-4:] == '2016':\n",
        "        break\n",
        "      containers = soup.find_all(\"h3\",{ \"class\":\"fc-item__title\"})\n",
        "      for items in containers:\n",
        "        h = '\"' + items.find(\"span\",{\"class\":\"js-headline-text\"}).text +'\"'\n",
        "        l = items.a[\"href\"]\n",
        "        f.write(line(h,l,0) + '\\n')\n",
        "    except:\n",
        "      print(site)\n",
        "      i=1\n",
        "      break\n",
        "    i+=1\n",
        "f.close()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.theguardian.com/world?page=1154\n",
            "https://www.theguardian.com/commentisfree?page=606\n",
            "https://www.theguardian.com/sport?page=1901\n",
            "https://www.theguardian.com/culture?page=1901\n",
            "https://www.theguardian.com/lifeandstyle?page=1463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB3va4mgw3Dv",
        "outputId": "569053c5-c6ad-46d2-a934-420cfa4b14d8"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import sys,time,random\n",
        "f = open(\"betootaadvocate.json\",'w')\n",
        "urls = [\"https://www.betootaadvocate.com/category/breaking-news/page/\",\"https://www.betootaadvocate.com/category/advocate-in-focus/page/\",\"https://www.betootaadvocate.com/category/entertainment/page/\",\"https://www.betootaadvocate.com/category/sports/page/\",\"https://www.betootaadvocate.com/category/uncategorized/page/\",\"https://www.betootaadvocate.com/category/humans-of-betoota/page/\",\"https://www.betootaadvocate.com/category/world-news/page/\"]\n",
        "i=1\n",
        "for url in urls:\n",
        "  while 1:\n",
        "    site = url + str(i)\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    try:\n",
        "      page = urlopen(req)\n",
        "      soup = BeautifulSoup(page)\n",
        "\n",
        "      containers = soup.find_all(\"h3\",{ \"class\":\"entry-title td-module-title\"})\n",
        "      for h3 in containers:\n",
        "        h = '\"' + h3.a[\"title\"] + '\"'\n",
        "        l = h3.a[\"href\"]\n",
        "        f.write(line(h,l,1) + '\\n')\n",
        "    except:\n",
        "      print(site)\n",
        "      i=1\n",
        "      break\n",
        "    i+=1\n",
        "f.close()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.betootaadvocate.com/category/uncategorized/page/452\n",
            "https://www.betootaadvocate.com/category/humans-of-betoota/page/161\n",
            "https://www.betootaadvocate.com/category/world-news/page/56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S65qYpIQRUIY",
        "outputId": "1d296ce4-d325-433c-9d87-bcfc72e82948"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import sys,time,random\n",
        "f = open(\"huzlers.json\",'w')\n",
        "urls = [\"https://www.huzlers.com/page/\"]\n",
        "i=1\n",
        "for url in urls:\n",
        "  while 1:\n",
        "    site = url + str(i)\n",
        "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
        "    req = Request(site,headers=hdr)\n",
        "    try:\n",
        "      page = urlopen(req)\n",
        "      soup = BeautifulSoup(page)\n",
        "\n",
        "      containers = soup.find_all(\"div\",{\"class\":\"entry-featured-media\"})\n",
        "      for div in containers:\n",
        "        h = '\"'  + div.a[\"title\"] + '\"'\n",
        "        l = div.a[\"href\"]\n",
        "        f.write(line(h,l,1) + '\\n')\n",
        "        \n",
        "    except:\n",
        "      print(site)\n",
        "      i=1\n",
        "      break\n",
        "    i+=1\n",
        "f.close()\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.huzlers.com/page/12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KciG3d-pV7Ty"
      },
      "source": [
        "**Quá trình thu thập dữ liệu:**\n",
        "- Với website https://babylonbee.com/ (1): Chỉ lấy article headlines ở https://babylonbee.com/news. Hàm babylonbee_articles viết bên trên nhận vào số trang muốn scrape và sẽ tiến hành chạy từ trang 1 đến trang đó, Như trên đây nhóm lấy 354 trang là hết tất cả các trang luôn rồi.\n",
        "- Với website https://thehardtimes.net/ (1): Tương tự như babylonbee cũng có một hàm babylonbee_articles nhận vào số trang muốn scrape và chạy từ trang 1 tới trang đó. Ở đây nhóm scrape tất cả các trang là 372 trang.\n",
        "- Với website https://www.nytimes.com/ (0): Ở đây nhóm scrape theo ngày tháng năm nhờ vào site map. URL có dạng như https://www.nytimes.com/sitemap/2021/06/07/. Nhóm đã scrape từ năm 2020 đến hết năm 2016\n",
        "- Với website https://www.thetimes.co.uk/ (0): Tương tự nytimes, the times cũng có sitemap nhưng URL có dạng https://www.thetimes.co.uk/html-sitemap/2020-01-2 với chỉ số cuối thay vì ngày là số từ 1 tới 4. Nhóm đã scrape từ năm 2020 đến hết năm 2016.\n",
        "- Với website https://www.ft.com/ (0): Nhóm scrape theo category urls = [\"https://www.ft.com/world?page=\",\"https://www.ft.com/companies?page=\",\"https://www.ft.com/technology?page=\",\"https://www.ft.com/markets?page=\",\"https://www.ft.com/climate-capital?page=\",\"https://www.ft.com/opinion?page=\",\"https://www.ft.com/work-careers?page=\",\"https://www.ft.com/arts?page=\",\"https://www.ft.com/books?page=\",\"https://www.ft.com/food-drink?page=\",\"https://www.ft.com/magazine?page=\",\"https://www.ft.com/house-home?page=\",\"https://www.ft.com/style?page=\",\"https://www.ft.com/travel?page=\",\"https://www.ft.com/htsi?page=\"], tiến hành chạy từ trang 1 đến khi nào lỗi thì chuyển sang category tiếp theo\n",
        "- Với website https://www.theguardian.com/ (0): cách làm y hệt như web https://www.ft.com/ với urls = [\"https://www.theguardian.com/world?page=\",\"https://www.theguardian.com/commentisfree?page=\",\"https://www.theguardian.com/sport?page=\",\"https://www.theguardian.com/culture?page=\",\"https://www.theguardian.com/lifeandstyle?page=\"]\n",
        "- Với website https://www.betootaadvocate.com/ (1): Cách làm cũng tương tự như 2 website trên là scrape theo category urls = [\"https://www.betootaadvocate.com/category/breaking-news/page/\",\"https://www.betootaadvocate.com/category/advocate-in-focus/page/\",\"https://www.betootaadvocate.com/category/entertainment/page/\",\"https://www.betootaadvocate.com/category/sports/page/\",\"https://www.betootaadvocate.com/category/uncategorized/page/\",\"https://www.betootaadvocate.com/category/humans-of-betoota/page/\",\"https://www.betootaadvocate.com/category/world-news/page/\"]\n",
        "- Với website https://www.huzlers.com/ (1): Scrape các trang có dạng https://www.huzlers.com/page/1/ với 1 là số trang và tăng dần đến khi lỗi thì dừng lại\n",
        "\n",
        "Các website khác nhau có cấu trúc html khác nhau nên muốn scrape phải xem page source của từng website, tìm element nào chứa headlines cũng như article url để lấy.\n",
        "\n",
        "Sau khi có tất cả các file .json rồi thì nhóm dùng Sublime Text để xóa các article trùng nhau trong file (Mở file bằng Sublime Text ->Edit->Permute Lines->Unique)\n",
        "\n",
        "Số headlines thu được:\n",
        "- Babylonbee (1): 7073\n",
        "- Betootaadvocate (1): 10236\n",
        "- Ft (0): 37853\n",
        "- Huzlers (1): 699\n",
        "- Newyorktimes (0): 296271\n",
        "- Theguardian (0): 130987\n",
        "- Thehardtime (1): 4176\n",
        "- Thetimes (0): 355287"
      ]
    }
  ]
}